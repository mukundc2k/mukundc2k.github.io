---
title:          "unveiLing: What Makes Anon Tricky for LLMs?"
date:           2025-03-29
selected:       true
pub:            "Under Review"
pub_date:       "2025"

abstract: >-
  Large language models (LLMs) have demonstrated potential in reasoning tasks, but their performance on anon remains consistently poor. Anon, often derived from Anon contests, provide a minimal contamination environment to assess LLMs' linguistic reasoning abilities across low-resource languages. In this work, we analyze LLMs' performance on 629 anon across 41 low-resource languages by labelling each with linguistically informed features to unveil weaknesses. Our analyses show that LLMs struggle with puzzles involving higher morphological complexity and perform better on anon involving linguistic features that are also found in English. We also show that splitting words into morphemes as a pre-processing step improves solvability, indicating a need for more informed and language-specific tokenisers. These findings thus offer insights into some challenges in linguistic reasoning and modelling of low-resource languages.
cover:          /assets/images/covers/figure6.png
authors:
  - Mukund Choudhary
  - K V Aditya Srivatsa
  - Gaurja Aeron
  - ...
  - Ekaterina Kochmar
  - Monojit Choudhury
---
