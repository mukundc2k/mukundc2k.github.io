---
title:          "Nanda Family: Open-Weights Generative Large Language Models for Hindi"
date:           2026-01-01
selected:       true
pub:            "EACL"
pub_date:       "2026"

abstract: >-
  LLMs for Hindi are just starting to grow, and are lacking quality especially for culturally grounded tasks. we help fix this for Hindi with Nanda (10B and 87B), adapting Llama-3/3.1 through three key moves: (1) extending the tokenizer with 20% Hindi-specific tokens to halve tokenization fertility while keeping English efficient, (2) Hindi-first parameter-efficient continuous pretraining on 65B tokens covering Devanagari, code-mixed, and romanized Hindi, and (3) bilingual instruction and safety alignment on culturally grounded data. Nanda models beat comparable open-weights and show top-tier safety performance. the takeaway: smart tokenizer design, data curation, and expansion-based continual pretraining can get you capable and safe LLMs for resource-poor languages without sacrificing English performance.
cover:          /assets/images/covers/nnd.png
authors:
  - Aaryamonvikram Singh#
  - ...
  - Monojit Choudhury
  - ...
  - Mukund Choudhary
  - ...
  - Preslav Nakov
---
